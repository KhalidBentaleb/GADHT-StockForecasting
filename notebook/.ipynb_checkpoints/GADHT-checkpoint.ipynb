{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80890c-1844-4a90-bcb8-fd2030449283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# GADHT - Centralized Setup, Imports, and Environment\n",
    "# =====================================================\n",
    "\n",
    "# ----------------------------\n",
    "# Standard Libraries\n",
    "# ----------------------------\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Dict, Any, Optional, Union\n",
    "\n",
    "# ----------------------------\n",
    "# Numerical Computation & Data\n",
    "# ----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# ----------------------------\n",
    "# Visualization\n",
    "# ----------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ----------------------------\n",
    "# Machine Learning / Deep Learning\n",
    "# ----------------------------\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset\n",
    "\n",
    "# ----------------------------\n",
    "# Time Series Decomposition\n",
    "# ----------------------------\n",
    "from PyEMD import CEEMDAN\n",
    "\n",
    "# ----------------------------\n",
    "# Preprocessing & Evaluation\n",
    "# ----------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ----------------------------\n",
    "# Explainability\n",
    "# ----------------------------\n",
    "import shap\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =====================================================\n",
    "# Global Configuration\n",
    "# =====================================================\n",
    "\n",
    "# Warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] [%(levelname)s] - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device Setup (CPU / GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"ðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "# Global Random Seed\n",
    "def set_global_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Set random seed across random, numpy, and torch.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    logger.info(f\"ðŸ”’ Global seed set to {seed}\")\n",
    "\n",
    "set_global_seed(42)\n",
    "\n",
    "# Version Information\n",
    "logger.info(\n",
    "    \"ðŸ“¦ Versions -> Torch: %s | NumPy: %s | Pandas: %s | Matplotlib: %s | Sklearn: %s | SHAP: %s | yfinance: %s | PyEMD: %s\",\n",
    "    torch.__version__, np.__version__, pd.__version__, plt.matplotlib.__version__,\n",
    "    \"sklearn\", shap.__version__, yf.__version__, CEEMDAN.__module__.split('.')[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e361e-f069-4c60-a49b-b4ac82a08b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GADHT - Multi-Phase Financial Data Downloader (Yahoo Finance)\n",
    "# ============================================================\n",
    "\n",
    "PHASE_TICKERS = {\n",
    "    \"pretraining\": [\n",
    "        \"NVDA\",  # NVIDIA Corp. (Technology)\n",
    "        \"JPM\",   # JPMorgan Chase (Financials)\n",
    "        \"XOM\",   # ExxonMobil (Energy)\n",
    "        \"LIN\",   # Linde plc (Materials)\n",
    "        \"PLD\",   # Prologis, Inc. (Real Estate)\n",
    "        \"AMZN\",  # Amazon.com Inc. (Consumer Discretionary)\n",
    "        \"PFE\",   # Pfizer Inc. (Health Care)\n",
    "        \"BA\",    # Boeing Co. (Industrials)\n",
    "    ],\n",
    "    \"finetuning\": [\n",
    "        \"TSLA\",  # Tesla, Inc. (Consumer Discretionary)\n",
    "        \"JNJ\",   # Johnson & Johnson (Health Care)\n",
    "        \"MSFT\",  # Microsoft Corp. (Technology)\n",
    "        \"NKE\",   # Nike, Inc. (Consumer Discretionary)\n",
    "        \"PG\",    # Procter & Gamble (Consumer Staples)\n",
    "        \"UNH\",   # UnitedHealth Group (Health Care)\n",
    "    ],\n",
    "    \"zeroshot\": [\n",
    "        \"META\",  # Meta Platforms (Communication Services)\n",
    "        \"KO\",    # Coca-Cola Co. (Consumer Staples)\n",
    "        \"CAT\",   # Caterpillar Inc. (Industrials)\n",
    "        \"BAC\",   # Bank of America (Financials)\n",
    "        \"AAPL\",  # Apple Inc. (Technology)\n",
    "        \"NEE\",   # NextEra Energy (Utilities)\n",
    "    ],\n",
    "}\n",
    "\n",
    "ALL_TICKERS = sorted({ticker for phase in PHASE_TICKERS.values() for ticker in phase})\n",
    "\n",
    "START_DATE = \"2005-01-01\"\n",
    "END_DATE = \"2025-05-31\"\n",
    "DATA_DIRECTORY = \"data/ohlcv\"\n",
    "\n",
    "import shutil\n",
    "if os.path.exists(DATA_DIRECTORY):\n",
    "    shutil.rmtree(DATA_DIRECTORY)\n",
    "os.makedirs(DATA_DIRECTORY, exist_ok=True)\n",
    "\n",
    "\n",
    "def download_ohlcv_data(ticker: str, start: str, end: str, save_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Download OHLCV data for a single ticker (sequential mode).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"ðŸ“¥ Downloading {ticker} ({start} â†’ {end})\")\n",
    "\n",
    "        df = yf.download(ticker, start=start, end=end, progress=False)[\n",
    "            [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "        ].dropna()\n",
    "\n",
    "        if df.empty:\n",
    "            logger.warning(f\"âš ï¸ {ticker}: No valid OHLCV data found. Skipping.\")\n",
    "            return\n",
    "\n",
    "        filepath = os.path.join(save_dir, f\"{ticker}.csv\")\n",
    "        df.to_csv(filepath, index=True, index_label=\"Date\")\n",
    "\n",
    "        logger.info(f\"âœ… {ticker}: {len(df)} rows saved â†’ {filepath}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ {ticker}: Error downloading ({e})\")\n",
    "\n",
    "\n",
    "logger.info(f\"ðŸš€ Starting SEQUENTIAL OHLCV download for {len(ALL_TICKERS)} tickers: {ALL_TICKERS}\")\n",
    "\n",
    "for ticker in ALL_TICKERS:\n",
    "    download_ohlcv_data(ticker, START_DATE, END_DATE, DATA_DIRECTORY)\n",
    "\n",
    "logger.info(\"ðŸ All OHLCV asset data downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe2812-3f74-4fba-9ea4-b2e2629d7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# GADHT - Adaptive Dataset with Sliding-Window CEEMDAN Caching\n",
    "# =============================================================\n",
    "\n",
    "class FinancialIMFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch dataset for GADHT model.\n",
    "    \n",
    "    Modes:\n",
    "    - Supervised mode: predict closing price H days ahead.\n",
    "    - Pretraining mode: randomly mask IMF components (self-supervised).\n",
    "    \n",
    "    Optimization:\n",
    "        Instead of recomputing CEEMDAN for every window (very slow),\n",
    "        we decompose the time series in overlapping segments (sliding-window CEEMDAN).\n",
    "        This preserves local adaptivity while avoiding redundant computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filepath: str,\n",
    "        window_size: int = 30,\n",
    "        max_imfs: int = 5,\n",
    "        use_ceemdan: bool = True,\n",
    "        pretraining_mode: bool = False,\n",
    "        normalize: bool = True,\n",
    "        prediction_horizon: int = 1,\n",
    "        segment_size: int = 90,\n",
    "        overlap: int = 30\n",
    "    ):\n",
    "        self.window_size = window_size\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.max_imfs = max_imfs\n",
    "        self.use_ceemdan = use_ceemdan\n",
    "        self.pretraining_mode = pretraining_mode\n",
    "        self.segment_size = segment_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "        # ----------------------------\n",
    "        # Load OHLCV data\n",
    "        # ----------------------------\n",
    "        df = pd.read_csv(filepath)\n",
    "        df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].dropna()\n",
    "        df = df.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "\n",
    "        if normalize:\n",
    "            scaler = StandardScaler()\n",
    "            df[df.columns] = scaler.fit_transform(df)\n",
    "\n",
    "        self.data = df.values  # shape: (T, d)\n",
    "\n",
    "        # ----------------------------\n",
    "        # CEEMDAN decomposition\n",
    "        # ----------------------------\n",
    "        if self.use_ceemdan:\n",
    "            self.imf_cache = self._precompute_imfs()\n",
    "        else:\n",
    "            self.imf_cache = None\n",
    "\n",
    "        # ----------------------------\n",
    "        # Build training samples\n",
    "        # ----------------------------\n",
    "        self.samples = self._build_sequences()\n",
    "\n",
    "    def _precompute_imfs(self) -> np.ndarray:\n",
    "        ceemdan = CEEMDAN()\n",
    "        T, d = self.data.shape\n",
    "        imf_cache = np.zeros((T, self.max_imfs, d), dtype=np.float32)\n",
    "\n",
    "        for feature_index in range(d):\n",
    "            signal = self.data[:, feature_index]\n",
    "            for start in range(0, T - self.segment_size + 1, self.segment_size - self.overlap):\n",
    "                end = start + self.segment_size\n",
    "                segment = signal[start:end]\n",
    "                imfs = ceemdan(segment)\n",
    "\n",
    "                if imfs.shape[0] < self.max_imfs:\n",
    "                    padding = self.max_imfs - imfs.shape[0]\n",
    "                    imfs = np.pad(imfs, ((0, padding), (0, 0)), mode=\"constant\")\n",
    "                else:\n",
    "                    imfs = imfs[:self.max_imfs]\n",
    "\n",
    "                seg_len = end - start\n",
    "                imf_cache[start:end, :, feature_index] = imfs.T[:seg_len]\n",
    "\n",
    "        return imf_cache  # (T, max_imfs, d)\n",
    "\n",
    "    def _build_sequences(self) -> List:\n",
    "        sequences = []\n",
    "        total_length = len(self.data)\n",
    "\n",
    "        for i in range(total_length - self.window_size - self.prediction_horizon + 1):\n",
    "            if self.use_ceemdan:\n",
    "                imf_cube = self.imf_cache[i:i+self.window_size, :, :]  # (T, M, F)\n",
    "                input_tensor = imf_cube\n",
    "            else:\n",
    "                window = self.data[i:i+self.window_size]  # (T, F)\n",
    "                input_tensor = np.expand_dims(window, axis=1)  # (T, 1, F)\n",
    "\n",
    "            target_close = self.data[i + self.window_size + self.prediction_horizon - 1, 3]\n",
    "            sequences.append((input_tensor.astype(np.float32), np.float32(target_close)))\n",
    "\n",
    "        return sequences\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        input_tensor, target = self.samples[idx]\n",
    "\n",
    "        if self.pretraining_mode:\n",
    "            # Full IMF cube as target\n",
    "            target_tensor = torch.tensor(input_tensor, dtype=torch.float32)  # (T, M, F)\n",
    "\n",
    "            # Start from clean input\n",
    "            masked_tensor = target_tensor.clone()\n",
    "\n",
    "            # Build mask (M, F)\n",
    "            mask = torch.ones(self.max_imfs, input_tensor.shape[-1], dtype=torch.float32)\n",
    "\n",
    "            # Randomly choose IMF indices to mask\n",
    "            num_masked = np.random.randint(1, 3)  # mask 1â€“2 IMFs\n",
    "            masked_indices = np.random.choice(self.max_imfs, size=num_masked, replace=False)\n",
    "\n",
    "            for m in masked_indices:\n",
    "                masked_tensor[:, m, :] = 0.0\n",
    "                mask[m, :] = 0.0\n",
    "\n",
    "            return (\n",
    "                masked_tensor,   # (T, M, F)\n",
    "                target_tensor,   # (T, M, F)\n",
    "                mask             # (M, F)\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            return (\n",
    "                torch.tensor(input_tensor, dtype=torch.float32),  # (T, M, F) or (T, 1, F)\n",
    "                torch.tensor(target, dtype=torch.float32)         # scalar\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6052dfd-3188-4f6e-92e5-fcbab636d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# GADHT - Dataset Loader and Safe Collation for DataLoader Batches\n",
    "# =============================================================\n",
    "\n",
    "def load_ticker_datasets(\n",
    "    tickers: List[str],\n",
    "    data_path: str = DATA_DIRECTORY,\n",
    "    window_size: int = 30,\n",
    "    max_imfs: int = 5,\n",
    "    use_ceemdan: bool = True,\n",
    "    pretraining_mode: bool = False,\n",
    "    normalize: bool = True,\n",
    "    prediction_horizon: int = 1\n",
    ") -> ConcatDataset:\n",
    "    \"\"\"\n",
    "    Load multiple FinancialIMFDataset instances from OHLCV CSV files.\n",
    "    Skips missing or empty datasets automatically.\n",
    "\n",
    "    Args:\n",
    "        tickers (List[str]): List of ticker symbols (e.g., ['AAPL', 'TSLA']).\n",
    "        data_path (str): Directory containing the OHLCV CSV files.\n",
    "        window_size (int): Sequence length per sample.\n",
    "        max_imfs (int): Number of IMFs per feature (if CEEMDAN enabled).\n",
    "        use_ceemdan (bool): Whether to apply CEEMDAN decomposition.\n",
    "        pretraining_mode (bool): If True, load in IMF-masking mode.\n",
    "        normalize (bool): Apply Z-score normalization per feature.\n",
    "        prediction_horizon (int): Days ahead to predict (default=1).\n",
    "\n",
    "    Returns:\n",
    "        ConcatDataset: Combined dataset of all valid tickers.\n",
    "    \"\"\"\n",
    "    logger.info(f\"ðŸ“¦ Loading datasets for {len(tickers)} tickers...\")\n",
    "    datasets = []\n",
    "\n",
    "    for ticker in tqdm(tickers, desc=\"Loading datasets\"):\n",
    "        filepath = os.path.join(data_path, f\"{ticker}.csv\")\n",
    "\n",
    "        if not os.path.isfile(filepath):\n",
    "            logger.warning(f\"âš ï¸ Missing file: {filepath}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            dataset = FinancialIMFDataset(\n",
    "                filepath=filepath,\n",
    "                window_size=window_size,\n",
    "                max_imfs=max_imfs,\n",
    "                use_ceemdan=use_ceemdan,\n",
    "                pretraining_mode=pretraining_mode,\n",
    "                normalize=normalize,\n",
    "                prediction_horizon=prediction_horizon\n",
    "            )\n",
    "\n",
    "            if len(dataset) == 0:\n",
    "                logger.warning(f\"âš ï¸ Empty dataset for {ticker}. Skipped.\")\n",
    "                continue\n",
    "\n",
    "            datasets.append(dataset)\n",
    "            logger.info(f\"âœ… {ticker} loaded with {len(dataset)} samples.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Failed to load {ticker}: {e}\")\n",
    "\n",
    "    if not datasets:\n",
    "        logger.critical(\"âŒ No valid datasets could be loaded.\")\n",
    "        raise RuntimeError(\"No datasets available. Check input files and configuration.\")\n",
    "\n",
    "    logger.info(f\"ðŸŽ¯ Successfully loaded {len(datasets)} datasets ({sum(len(ds) for ds in datasets)} total samples).\")\n",
    "    return ConcatDataset(datasets)\n",
    "\n",
    "\n",
    "def safe_collate(batch: List) -> Union[dict, torch.Tensor, None]:\n",
    "    \"\"\"\n",
    "    Custom collate function for DataLoader that ignores None samples\n",
    "    and handles both supervised and pretraining modes.\n",
    "\n",
    "    Returns:\n",
    "        - Pretraining mode: dict with {\"masked\", \"target\", \"mask\"}\n",
    "        - Supervised mode: dict with {\"input\", \"target\"}\n",
    "    \"\"\"\n",
    "    # Filter out None values\n",
    "    batch = [sample for sample in batch if sample is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "\n",
    "    # Pretraining mode: (masked_tensor, original_tensor, mask)\n",
    "    if isinstance(batch[0], tuple) and len(batch[0]) == 3:\n",
    "        masked_tensors, originals, masks = zip(*batch)\n",
    "        return {\n",
    "            \"masked\": torch.stack(masked_tensors),\n",
    "            \"target\": torch.stack(originals),\n",
    "            \"mask\": torch.stack(masks)\n",
    "        }\n",
    "\n",
    "    # Supervised mode: (input_tensor, target)\n",
    "    if isinstance(batch[0], tuple) and len(batch[0]) == 2:\n",
    "        inputs, targets = zip(*batch)\n",
    "        return {\n",
    "            \"input\": torch.stack(inputs),\n",
    "            \"target\": torch.stack(targets)\n",
    "        }\n",
    "\n",
    "    # Fallback â†’ default PyTorch behavior\n",
    "    return torch.utils.data.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6aba96-1bba-4e42-917b-3ba6f3b2e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# GADHT - Energy-Aware Multihead Attention\n",
    "# ==========================================================\n",
    "\n",
    "class EnergyAwareMultiheadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention layer with spectral energy weighting.\n",
    "\n",
    "    Instead of simply adding energy to the output, this module \n",
    "    multiplies the attention logits by IMF-specific energy values \n",
    "    before the softmax, as described in the GADHT paper (Eq. 12).\n",
    "\n",
    "    Args:\n",
    "        d_model (int): Embedding dimension of inputs.\n",
    "        num_heads (int): Number of attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.o_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,              # (B, T, d_model)\n",
    "        energy: Optional[torch.Tensor] = None  # (B, T, 1) or (B, T)\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # Project Q, K, V\n",
    "        Q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, h, T, d_k)\n",
    "        K = self.k_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, h, T, d_k)\n",
    "        V = self.v_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, h, T, d_k)\n",
    "\n",
    "        # Compute scaled dot-product attention logits\n",
    "        attn_logits = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, h, T, T)\n",
    "\n",
    "        # Apply spectral energy weighting (broadcast along heads)\n",
    "        if energy is not None:\n",
    "            if energy.dim() == 2:\n",
    "                energy = energy.unsqueeze(-1)  # (B, T, 1)\n",
    "            # Expand to match attention logits\n",
    "            energy = energy.unsqueeze(1)  # (B, 1, T, 1)\n",
    "            attn_logits = attn_logits * energy\n",
    "\n",
    "        # Softmax over keys\n",
    "        attn_weights = torch.softmax(attn_logits, dim=-1)  # (B, h, T, T)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = torch.matmul(attn_weights, V)  # (B, h, T, d_k)\n",
    "\n",
    "        # Reshape back\n",
    "        output = output.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "        output = self.o_proj(output)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1b4ff-c015-4b95-86cf-89456f022308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# GADHT - Generative Adaptive Decomposition Hierarchical Transformer\n",
    "# ==========================================================\n",
    "\n",
    "class GADHTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GADHT: Generative Adaptive Decomposition Hierarchical Transformer.\n",
    "\n",
    "    Modes:\n",
    "      - Supervised forecasting: predict next-day closing price.\n",
    "      - Pretraining: masked IMF reconstruction (reconstruct full (T, M, F) cube).\n",
    "\n",
    "    Args:\n",
    "        input_shape (Tuple[int, int, int]): (time_steps, num_imfs, num_features).\n",
    "        d_model (int): Embedding dimension for Transformer.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        num_layers (int): Number of temporal Transformer layers (per-IMF).\n",
    "        dropout (float): Dropout rate.\n",
    "        use_energy (bool): If True, use energy-weighted spectral attention.\n",
    "        pretraining (bool): If True, model outputs IMF reconstruction instead of forecast.\n",
    "        spectral_layers (int): Number of spectral attention layers across IMFs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int, int, int] = (30, 5, 5),\n",
    "        d_model: int = 128,\n",
    "        num_heads: int = 4,\n",
    "        num_layers: int = 3,\n",
    "        dropout: float = 0.1,\n",
    "        use_energy: bool = True,\n",
    "        pretraining: bool = False,\n",
    "        spectral_layers: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len, self.num_imfs, self.num_features = input_shape\n",
    "        self.d_model = d_model\n",
    "        self.pretraining = pretraining\n",
    "        self.use_energy = use_energy\n",
    "        self.spectral_layers = spectral_layers\n",
    "\n",
    "        # ----------------------------\n",
    "        # Input embedding (feature -> d_model), shared across IMFs\n",
    "        # ----------------------------\n",
    "        self.input_proj = nn.Linear(self.num_features, d_model)\n",
    "\n",
    "        # Fixed sinusoidal positional encoding (temporal)\n",
    "        self.register_buffer(\"positional_encoding\",\n",
    "            self._build_positional_encoding(self.seq_len, d_model),\n",
    "            persistent=False\n",
    "        )\n",
    "\n",
    "        # ----------------------------\n",
    "        # Temporal Encoder (per-IMF): shared parameters for all IMFs\n",
    "        # We reuse EnergyAwareMultiheadAttention with energy=None here.\n",
    "        # ----------------------------\n",
    "        self.temporal_layers = nn.ModuleList([\n",
    "            EnergyAwareMultiheadAttention(d_model, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.temporal_norm = nn.LayerNorm(d_model)\n",
    "        self.temporal_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Spectral Encoder (across IMFs): energy-weighted attention over IMF tokens\n",
    "        # Sequence length here is M (number of IMFs).\n",
    "        # ----------------------------\n",
    "        self.spectral_layers_mod = nn.ModuleList([\n",
    "            EnergyAwareMultiheadAttention(d_model, num_heads) for _ in range(max(1, spectral_layers))\n",
    "        ])\n",
    "        self.spectral_norm = nn.LayerNorm(d_model)\n",
    "        self.spectral_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Output heads\n",
    "        # ----------------------------\n",
    "        if pretraining:\n",
    "            # Decoder back to features for each time step and each IMF\n",
    "            self.decoder = nn.Linear(d_model, self.num_features)\n",
    "        else:\n",
    "            # Forecast head (global pooled representation -> scalar)\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(d_model, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(32, 1)\n",
    "            )\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    # ======================================================\n",
    "    # Initialization\n",
    "    # ======================================================\n",
    "    def _initialize_weights(self) -> None:\n",
    "        \"\"\"Xavier initialization for linear layers.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def _build_positional_encoding(self, seq_len: int, d_model: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Build fixed sinusoidal positional encoding (temporal).\n",
    "        Returns: (1, T, d_model)\n",
    "        \"\"\"\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, T, d_model)\n",
    "\n",
    "    # ======================================================\n",
    "    # Forward Pass\n",
    "    # ======================================================\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: (B, T, M, F)\n",
    "\n",
    "        Returns:\n",
    "            - Pretraining: reconstruction tensor (B, T, M, F)\n",
    "            - Forecasting: scalar prediction (B, 1)\n",
    "        \"\"\"\n",
    "        B, T, M, F = x.shape\n",
    "        assert T == self.seq_len and M == self.num_imfs and F == self.num_features, \\\n",
    "            f\"Input shape mismatch: expected {(self.seq_len, self.num_imfs, self.num_features)}, got {(T, M, F)}\"\n",
    "\n",
    "        # ----------------------------\n",
    "        # Compute IMF energy per sample (Eq. 10)\n",
    "        # e_m = (1 / (T*F)) * sum_{t,j} x[t,m,j]^2\n",
    "        # Shape: (B, M)\n",
    "        # ----------------------------\n",
    "        with torch.no_grad():\n",
    "            energy = (x.pow(2).mean(dim=(1, 3)))  # (B, M)\n",
    "            # Normalize energies for stability (softmax over IMFs)\n",
    "            energy = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Temporal encoding per IMF\n",
    "        # Process each IMF as a separate sequence of length T, with shared weights.\n",
    "        # ----------------------------\n",
    "        # Reshape to (B*M, T, F) then project to (B*M, T, d_model)\n",
    "        x_tm = x.permute(0, 2, 1, 3).contiguous().view(B * M, T, F)  # (B*M, T, F)\n",
    "        x_tm = self.input_proj(x_tm)  # (B*M, T, d_model)\n",
    "\n",
    "        # Add temporal positional encoding\n",
    "        x_tm = x_tm + self.positional_encoding.to(x_tm.device)\n",
    "\n",
    "        # Shared temporal Transformer layers (energy=None)\n",
    "        for layer in self.temporal_layers:\n",
    "            x_tm, _ = layer(x_tm, energy=None)  # (B*M, T, d_model)\n",
    "            x_tm = self.temporal_dropout(x_tm)\n",
    "\n",
    "        # Temporal pooling -> per-IMF embeddings: (B*M, d_model) -> (B, M, d_model)\n",
    "        x_tm = self.temporal_norm(x_tm.mean(dim=1))  # (B*M, d_model)\n",
    "        x_spec = x_tm.view(B, M, self.d_model)       # (B, M, d_model)\n",
    "\n",
    "        # If pretraining, we need full (T, M, F) reconstruction.\n",
    "        # Decode from temporal features BEFORE pooling using a lightweight decoder.\n",
    "        if self.pretraining:\n",
    "            # Re-run a small linear decoder on the sequence-level features x_tm_full\n",
    "            # For better recon, we decode from the last temporal hidden states per time step.\n",
    "            # We reuse the temporal sequence features computed above by\n",
    "            # recomputing the temporal path without mean-pooling:\n",
    "            x_seq = x.permute(0, 2, 1, 3).contiguous().view(B * M, T, F)   # (B*M, T, F)\n",
    "            x_seq = self.input_proj(x_seq) + self.positional_encoding.to(x_seq.device)  # (B*M, T, d_model)\n",
    "            for layer in self.temporal_layers:\n",
    "                x_seq, _ = layer(x_seq, energy=None)  # (B*M, T, d_model)\n",
    "\n",
    "            # Decode to features per time step\n",
    "            rec = self.decoder(x_seq)  # (B*M, T, F)\n",
    "            rec = rec.view(B, M, T, F).permute(0, 2, 1, 3).contiguous()  # (B, T, M, F)\n",
    "            return rec\n",
    "\n",
    "        # ----------------------------\n",
    "        # Spectral encoding across IMFs (energy-weighted attention over M tokens)\n",
    "        # x_spec: (B, M, d_model), energy: (B, M)\n",
    "        # ----------------------------\n",
    "        for layer in self.spectral_layers_mod:\n",
    "            x_spec, _ = layer(x_spec, energy=energy)  # (B, M, d_model)\n",
    "            x_spec = self.spectral_dropout(x_spec)\n",
    "\n",
    "        x_spec = self.spectral_norm(x_spec)  # (B, M, d_model)\n",
    "\n",
    "        # Spectral pooling -> global representation\n",
    "        x_global = x_spec.mean(dim=1)  # (B, d_model)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Forecast head\n",
    "        # ----------------------------\n",
    "        out = self.head(x_global)  # (B, 1)\n",
    "        return out\n",
    "\n",
    "    # ======================================================\n",
    "    # Encoder-only mode (returns latent vector)\n",
    "    # ======================================================\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encoder-only forward pass that returns a pooled latent vector.\n",
    "        Args:\n",
    "            x: (B, T, M, F)\n",
    "        Returns:\n",
    "            (B, d_model)\n",
    "        \"\"\"\n",
    "        B, T, M, F = x.shape\n",
    "\n",
    "        with torch.no_grad():\n",
    "            energy = torch.softmax((x.pow(2).mean(dim=(1, 3))), dim=-1)  # (B, M)\n",
    "\n",
    "        # Temporal path\n",
    "        x_tm = x.permute(0, 2, 1, 3).contiguous().view(B * M, T, F)\n",
    "        x_tm = self.input_proj(x_tm) + self.positional_encoding.to(x_tm.device)\n",
    "        for layer in self.temporal_layers:\n",
    "            x_tm, _ = layer(x_tm, energy=None)\n",
    "        x_tm = self.temporal_norm(x_tm.mean(dim=1))  # (B*M, d_model)\n",
    "        x_spec = x_tm.view(B, M, self.d_model)\n",
    "\n",
    "        # Spectral path\n",
    "        for layer in self.spectral_layers_mod:\n",
    "            x_spec, _ = layer(x_spec, energy=energy)\n",
    "        x_spec = self.spectral_norm(x_spec)\n",
    "\n",
    "        # Global pooled latent\n",
    "        return x_spec.mean(dim=1)  # (B, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be95674-17cb-45a9-b0bb-8c72122f4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# GADHT - Pretraining Wrapper for Masked IMF Reconstruction\n",
    "# ===========================================================\n",
    "\n",
    "class GADHTPretrainingWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for self-supervised pretraining via masked IMF reconstruction.\n",
    "\n",
    "    This module wraps a GADHT model (initialized with pretraining=True) \n",
    "    and ensures the forward pass outputs a reconstruction of the full \n",
    "    (T, M, F) cube as required by the reconstruction loss in the paper.\n",
    "\n",
    "    Args:\n",
    "        encoder (GADHTModel): Base GADHT model with `pretraining=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder: GADHTModel):\n",
    "        super().__init__()\n",
    "        if not encoder.pretraining:\n",
    "            raise ValueError(\"Encoder must be initialized with pretraining=True\")\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def forward(self, x_masked: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for masked IMF reconstruction.\n",
    "\n",
    "        Args:\n",
    "            x_masked (Tensor): Masked sequence of shape (B, T, M, F).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Reconstructed sequence of shape (B, T, M, F).\n",
    "        \"\"\"\n",
    "        return self.encoder(x_masked)\n",
    "\n",
    "def reconstruction_loss(x_reconstructed, x_original, mask):\n",
    "    \"\"\"\n",
    "    Masked MSE reconstruction loss.\n",
    "\n",
    "    Args:\n",
    "        x_reconstructed, x_original: (B, T, M, F)\n",
    "        mask: (B, M, 1, 1) binary mask of which IMFs were kept (1) or masked (0).\n",
    "    \"\"\"\n",
    "    masked_diff = (x_reconstructed - x_original) * mask.unsqueeze(1)  # broadcast over T and F\n",
    "    return (masked_diff ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79124a63-e953-4c2f-9283-7e53de9c008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# GADHT - Pretraining Loop for IMF Masking Reconstruction\n",
    "# ===========================================================\n",
    "\n",
    "def run_gadht_pretraining(\n",
    "    model: nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    num_epochs: int = 150,\n",
    "    learning_rate: float = 1e-5,\n",
    "    early_stopping_patience: int = 10,\n",
    "    max_gradient_norm: float = 1.0,\n",
    "    checkpoint_path: str = \"checkpoints/pretraining/gadht_pretrain.pth\",\n",
    "    plot_loss_curve: bool = True,\n",
    "    loss_plot_path: str = \"results/pretraining/gadht_pretrain_loss.png\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Self-supervised pretraining loop for GADHT (masked IMF reconstruction).\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): GADHTPretrainingWrapper (with encoder.pretraining=True).\n",
    "        dataloader (DataLoader): Training batches with dict:\n",
    "                                 {\"masked\": ..., \"target\": ..., \"mask\": ...}.\n",
    "        num_epochs (int): Maximum number of training epochs.\n",
    "        learning_rate (float): Optimizer learning rate.\n",
    "        early_stopping_patience (int): Stop training if no improvement after N epochs.\n",
    "        max_gradient_norm (float): Gradient clipping threshold.\n",
    "        checkpoint_path (str): File path to save best model weights.\n",
    "        plot_loss_curve (bool): Whether to save loss curve as PNG.\n",
    "        loss_plot_path (str): Path to save the loss curve plot.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    epoch_losses = []\n",
    "\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(loss_plot_path), exist_ok=True)\n",
    "\n",
    "    logger.info(f\"ðŸ”§ Starting GADHT pretraining for {num_epochs} epochs...\")\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=f\"[Epoch {epoch}/{num_epochs}]\"):\n",
    "            masked_inputs = batch[\"masked\"].to(device)   # (B, T, M, F) with masked IMFs\n",
    "            targets = batch[\"target\"].to(device)         # (B, T, M, F) original sequence\n",
    "            mask = batch[\"mask\"].to(device)              # (B, M, F) or (B, M)\n",
    "\n",
    "            # Forward\n",
    "            reconstructions = model(masked_inputs)       # (B, T, M, F)\n",
    "\n",
    "            # --- Fix: align mask dimensions with reconstructions ---\n",
    "            if mask.dim() == 2:  # (B, M)\n",
    "                mask = mask.unsqueeze(-1)  # (B, M, 1)\n",
    "            if mask.dim() == 3:  # (B, M, F)\n",
    "                mask = mask.unsqueeze(1)  # (B, 1, M, F)\n",
    "            mask = mask.expand(-1, targets.size(1), -1, -1)  # (B, T, M, F)\n",
    "\n",
    "            # Masked reconstruction loss: only on masked IMFs\n",
    "            masked_diff = (reconstructions - targets) * (1 - mask)\n",
    "            loss = (masked_diff ** 2).mean()\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if max_gradient_norm:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        logger.info(f\"ðŸ“‰ Epoch {epoch} - Avg MSE: {avg_loss:.6f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            logger.info(f\"âœ… Best model updated at epoch {epoch} (loss={best_loss:.6f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            logger.info(f\"â³ Early stopping patience: {patience_counter}/{early_stopping_patience}\")\n",
    "\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            logger.info(\"ðŸ›‘ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    logger.info(f\"ðŸ Pretraining complete. Best MSE: {best_loss:.6f}\")\n",
    "\n",
    "    # Plot loss curve\n",
    "    if plot_loss_curve:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(epoch_losses, label=\"Reconstruction Loss\")\n",
    "        plt.title(\"GADHT Pretraining Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"MSE Loss\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(loss_plot_path)\n",
    "        plt.close()\n",
    "        logger.info(f\"ðŸ“Š Loss curve saved to {loss_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd456a-7a0c-4022-8c74-1b570216fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GADHT - Fine-Tuning & Evaluation Pipeline (Final Version)\n",
    "# ============================================================\n",
    "\n",
    "def run_gadht_finetune(\n",
    "    model: nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    epochs: int = 150,\n",
    "    lr: float = 1e-5,\n",
    "    patience: int = 10,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    save_path: Optional[str] = None,\n",
    "    log_metrics: bool = True,\n",
    "    plot_curve: bool = True,\n",
    "    fig_path: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Fine-tune GADHT model on Close@t+1 forecasting with early stopping.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): GADHT model with forecasting head.\n",
    "        dataloader (DataLoader): Training data loader (dict with \"input\", \"target\").\n",
    "        epochs (int): Maximum training epochs.\n",
    "        lr (float): Learning rate.\n",
    "        patience (int): Early stopping patience.\n",
    "        max_grad_norm (float): Gradient clipping threshold.\n",
    "        save_path (str, optional): File path to save best model weights.\n",
    "        log_metrics (bool): Whether to log metrics each epoch.\n",
    "        plot_curve (bool): Whether to save RMSE learning curve.\n",
    "        fig_path (str, optional): Path to save RMSE curve figure.\n",
    "    \"\"\"\n",
    "    # ----------------------------\n",
    "    # Setup\n",
    "    # ----------------------------\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    best_rmse = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    rmse_history = []\n",
    "\n",
    "    # Ensure output directories exist\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    if fig_path:\n",
    "        os.makedirs(os.path.dirname(fig_path), exist_ok=True)\n",
    "\n",
    "    logger.info(f\"ðŸš€ Starting GADHT fine-tuning for {epochs} epochs...\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Training loop\n",
    "    # ----------------------------\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        valid_batches = 0\n",
    "        preds_all, targets_all = [], []\n",
    "\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\", leave=False)\n",
    "        for batch in pbar:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            try:\n",
    "                # Use dict keys from safe_collate\n",
    "                x, targets = batch[\"input\"].to(device), batch[\"target\"].to(device).unsqueeze(1)\n",
    "\n",
    "                # Forward + Loss\n",
    "                outputs = model(x)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "\n",
    "                # Backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                if max_grad_norm:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "                # Metrics accumulation\n",
    "                epoch_loss += loss.item()\n",
    "                valid_batches += 1\n",
    "                preds_all.append(outputs.detach().cpu().numpy())\n",
    "                targets_all.append(targets.detach().cpu().numpy())\n",
    "\n",
    "                pbar.set_postfix({\"loss\": f\"{loss.item():.6f}\"})\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âš ï¸ Skipped batch due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # ----------------------------\n",
    "        # End of epoch evaluation\n",
    "        # ----------------------------\n",
    "        if valid_batches > 0:\n",
    "            preds = np.concatenate(preds_all).flatten()\n",
    "            targets = np.concatenate(targets_all).flatten()\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(targets, preds))\n",
    "            mae = mean_absolute_error(targets, preds)\n",
    "            mape = np.mean(np.abs((targets - preds) / targets)) * 100\n",
    "            r2 = r2_score(targets, preds)\n",
    "            avg_loss = epoch_loss / valid_batches\n",
    "            rmse_history.append(rmse)\n",
    "\n",
    "            if log_metrics:\n",
    "                logger.info(\n",
    "                    f\"ðŸ“‰ Epoch {epoch} â€” Loss: {avg_loss:.6f} | \"\n",
    "                    f\"RMSE: {rmse:.4f} | MAE: {mae:.4f} | \"\n",
    "                    f\"MAPE: {mape:.2f}% | RÂ²: {r2:.4f}\"\n",
    "                )\n",
    "\n",
    "            # Save best model\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                patience_counter = 0\n",
    "                if save_path:\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "                    logger.info(f\"âœ… Best model saved with RMSE {best_rmse:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                logger.info(f\"â³ Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                logger.info(\"ðŸ›‘ Early stopping triggered.\")\n",
    "                break\n",
    "        else:\n",
    "            logger.warning(f\"âŒ Epoch {epoch}: No valid batches.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Training complete\n",
    "    # ----------------------------\n",
    "    logger.info(f\"ðŸ Fine-tuning complete. Best RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Plot learning curve\n",
    "    # ----------------------------\n",
    "    if plot_curve and len(rmse_history) > 1 and fig_path:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(rmse_history, label=\"RMSE\", linewidth=2)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"RMSE\")\n",
    "        plt.title(\"GADHT Fine-Tuning RMSE Curve\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_path)\n",
    "        plt.close()\n",
    "        logger.info(f\"ðŸ“Š RMSE curve saved to {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a52bb-73f2-4a78-8dc6-f92b3317c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# ðŸ“ˆ Backtesting Utilities\n",
    "# =====================================================\n",
    "\n",
    "def collect_predictions(model: nn.Module, dataloader: torch.utils.data.DataLoader, device: torch.device = device):\n",
    "    \"\"\"\n",
    "    Run inference over a dataloader and return flat numpy arrays.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained forecasting model.\n",
    "        dataloader (DataLoader): DataLoader providing {\"input\", \"target\"} batches.\n",
    "        device (torch.device): Device to run inference on.\n",
    "    \n",
    "    Returns:\n",
    "        preds (np.ndarray): Model predictions, shape (N,).\n",
    "        reals (np.ndarray): Ground-truth targets, shape (N,).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            # Use dict keys from safe_collate\n",
    "            x, y = batch[\"input\"].to(device), batch[\"target\"].to(device)\n",
    "\n",
    "            y_hat = model(x).squeeze()\n",
    "            all_preds.append(y_hat.detach().cpu().numpy())\n",
    "            all_true.append(y.detach().cpu().numpy())\n",
    "\n",
    "    if not all_preds or not all_true:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    preds = np.concatenate(all_preds).ravel()\n",
    "    reals = np.concatenate(all_true).ravel()\n",
    "    return preds, reals\n",
    "\n",
    "\n",
    "def backtest_strategy(preds: np.ndarray, reals: np.ndarray, cost_bps: float = 20, freq: int = 252) -> dict:\n",
    "    \"\"\"\n",
    "    Simple long/short daily backtest driven by predicted price direction.\n",
    "    \n",
    "    Strategy:\n",
    "        - Signal = sign of predicted price change (Î” preds).\n",
    "        - PnL = signal Ã— realized return.\n",
    "        - Transaction costs applied when signal changes (entry, exit, flip).\n",
    "    \n",
    "    Args:\n",
    "        preds (np.ndarray): Predicted prices (aligned with targets), shape (T,).\n",
    "        reals (np.ndarray): Realized prices (true values), shape (T,).\n",
    "        cost_bps (float): Transaction cost per trade in basis points (20 = 0.20%).\n",
    "        freq (int): Trading days per year for annualization (default 252).\n",
    "    \n",
    "    Returns:\n",
    "        dict with:\n",
    "            - Annualized Return\n",
    "            - Annualized Volatility\n",
    "            - Sharpe\n",
    "            - Max Drawdown\n",
    "            - Win Rate\n",
    "    \"\"\"\n",
    "    if len(preds) != len(reals) or len(preds) < 2:\n",
    "        return {\n",
    "            \"Annualized Return\": 0.0,\n",
    "            \"Annualized Volatility\": 0.0,\n",
    "            \"Sharpe\": 0.0,\n",
    "            \"Max Drawdown\": 0.0,\n",
    "            \"Win Rate\": 0.0,\n",
    "        }\n",
    "\n",
    "    # Realized returns\n",
    "    real_rets = np.diff(reals) / reals[:-1]  # (T-1,)\n",
    "\n",
    "    # Predicted signal (direction of price change)\n",
    "    sig = np.sign(np.diff(preds))            # (T-1,)\n",
    "    sig = np.where(sig == 0, 0, sig)         # flat if exactly zero\n",
    "\n",
    "    # Strategy returns (before costs)\n",
    "    strat = sig * real_rets\n",
    "\n",
    "    # Transaction costs (per switch or flip)\n",
    "    per_trade_cost = cost_bps / 10000.0\n",
    "    costs = per_trade_cost * np.abs(np.diff(sig, prepend=0))\n",
    "    strat -= costs\n",
    "\n",
    "    # Equity curve\n",
    "    equity = np.cumprod(1.0 + strat)\n",
    "    if equity.size == 0:\n",
    "        return {\n",
    "            \"Annualized Return\": 0.0,\n",
    "            \"Annualized Volatility\": 0.0,\n",
    "            \"Sharpe\": 0.0,\n",
    "            \"Max Drawdown\": 0.0,\n",
    "            \"Win Rate\": 0.0,\n",
    "        }\n",
    "\n",
    "    # Max drawdown\n",
    "    peak = np.maximum.accumulate(equity)\n",
    "    drawdown = equity / peak - 1.0\n",
    "    max_dd = float(drawdown.min())\n",
    "\n",
    "    # Annualized metrics\n",
    "    mu = strat.mean()\n",
    "    sigma = strat.std()\n",
    "    ann_ret = float(mu * freq)\n",
    "    ann_vol = float(sigma * np.sqrt(freq))\n",
    "    sharpe = float(ann_ret / ann_vol) if ann_vol > 0 else 0.0\n",
    "    win_rate = float((strat > 0).mean())\n",
    "\n",
    "    return {\n",
    "        \"Annualized Return\": ann_ret,\n",
    "        \"Annualized Volatility\": ann_vol,\n",
    "        \"Sharpe\": sharpe,\n",
    "        \"Max Drawdown\": max_dd,\n",
    "        \"Win Rate\": win_rate,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58818af9-b74b-41e1-9ac4-6d2b379a27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# GADHT - Rolling-Origin Cross-Validation Pipeline\n",
    "# ===========================================================\n",
    "\n",
    "def run_gadht_rolling_cv(\n",
    "    tickers: List[str],\n",
    "    model_name: str,\n",
    "    dataset_name: str,\n",
    "    pretrained_path: str,\n",
    "    n_splits: int = 5,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 1e-5,\n",
    "    epochs: int = 150,\n",
    "    patience: int = 10,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    seed: int = 42,\n",
    "    prediction_horizon: int = 1\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Execute GADHT fine-tuning with rolling-origin cross-validation.\n",
    "    \n",
    "    Each fold preserves chronological order to avoid lookahead bias,\n",
    "    and evaluates both statistical and financial metrics:\n",
    "        - Statistical: RMSE, MAE, MAPE, RÂ²\n",
    "        - Financial: Annualized Return, Volatility, Sharpe, MaxDD, WinRate\n",
    "    \"\"\"\n",
    "    logger.info(\n",
    "        f\"ðŸš€ Starting GADHT {n_splits}-fold rolling CV \"\n",
    "        f\"on {dataset_name} | Horizon={prediction_horizon}\"\n",
    "    )\n",
    "    os.makedirs(\"results/finetune\", exist_ok=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dataset loading\n",
    "    # ----------------------------\n",
    "    dataset = load_ticker_datasets(\n",
    "        tickers=tickers,\n",
    "        data_path=DATA_DIRECTORY,\n",
    "        window_size=30,\n",
    "        max_imfs=5,\n",
    "        use_ceemdan=True,\n",
    "        pretraining_mode=False,\n",
    "        normalize=True,\n",
    "        prediction_horizon=prediction_horizon\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # Rolling-origin CV definition (chronological, no shuffle!)\n",
    "    # ----------------------------\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(dataset)):\n",
    "        logger.info(\n",
    "            f\"ðŸŒ€ Fold {fold_idx+1}/{n_splits} | \"\n",
    "            f\"Train: {len(train_idx)} | Val: {len(val_idx)}\"\n",
    "        )\n",
    "\n",
    "        # DataLoaders\n",
    "        train_loader = DataLoader(\n",
    "            Subset(dataset, train_idx),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,   # preserve time order\n",
    "            collate_fn=safe_collate\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            Subset(dataset, val_idx),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=safe_collate\n",
    "        )\n",
    "\n",
    "        # ----------------------------\n",
    "        # Model loading\n",
    "        # ----------------------------\n",
    "        model = GADHTModel(pretraining=False).to(device)\n",
    "        state_dict = torch.load(pretrained_path, map_location=device)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Fine-tuning\n",
    "        # ----------------------------\n",
    "        run_gadht_finetune(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            patience=patience,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            save_path=None\n",
    "        )\n",
    "\n",
    "        # ----------------------------\n",
    "        # Evaluation (val fold)\n",
    "        # ----------------------------\n",
    "        preds, reals = collect_predictions(model, val_loader, device=device)\n",
    "\n",
    "        if len(preds) == 0:\n",
    "            logger.warning(f\"âŒ Fold {fold_idx+1}: no predictions collected.\")\n",
    "            continue\n",
    "\n",
    "        # Statistical metrics\n",
    "        rmse = np.sqrt(mean_squared_error(reals, preds))\n",
    "        mae = mean_absolute_error(reals, preds)\n",
    "        mape = np.mean(np.abs((reals - preds) / reals)) * 100\n",
    "        r2 = r2_score(reals, preds)\n",
    "        metrics = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"MAPE\": mape,\n",
    "            \"RÂ²\": r2,\n",
    "        }\n",
    "\n",
    "        # Financial metrics\n",
    "        bt_metrics = backtest_strategy(preds, reals, cost_bps=20)\n",
    "        metrics.update(bt_metrics)\n",
    "\n",
    "        logger.info(f\"ðŸ“Š Fold {fold_idx+1} metrics: {metrics}\")\n",
    "        fold_metrics.append(metrics)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Save aggregated metrics\n",
    "    # ----------------------------\n",
    "    if fold_metrics:\n",
    "        df = pd.DataFrame(fold_metrics)\n",
    "        csv_path = f\"results/finetune/{model_name}_cv_metrics.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        logger.info(f\"âœ… Saved CV metrics to {csv_path}\")\n",
    "\n",
    "        # Plot error bars\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.errorbar(df.columns, df.mean(), yerr=df.std(), fmt=\"o\", capsize=5)\n",
    "        plt.title(\n",
    "            f\"{dataset_name} â€” {n_splits}-Fold Rolling-Origin CV \"\n",
    "            f\"(Horizon={prediction_horizon})\"\n",
    "        )\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plot_path = f\"results/finetune/{model_name}_cv_plot.png\"\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        logger.info(f\"ðŸ“Š CV plot saved to {plot_path}\")\n",
    "    else:\n",
    "        logger.warning(\"âŒ No metrics collected during CV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e0af4-45ed-4feb-96b0-26b598f54933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# GADHT - Model Evaluation for Close@t+H Forecasting\n",
    "# ===========================================================\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    return_predictions: bool = False,\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    cost_bps: float = 20\n",
    ") -> Union[Dict[str, float], Tuple[Dict[str, float], np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Evaluate a trained GADHT model on a labeled dataset.\n",
    "\n",
    "    Computes:\n",
    "        - Statistical metrics: RMSE, MAE, MAPE, RÂ²\n",
    "        - Financial metrics (via backtest): Annualized Return, Volatility, Sharpe, MaxDD, WinRate\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained GADHT model (forecasting mode).\n",
    "        dataloader (DataLoader): Evaluation data loader (dict with \"input\", \"target\").\n",
    "        return_predictions (bool): If True, also return y_pred and y_true arrays.\n",
    "        device (torch.device): Inference device (default = CUDA if available).\n",
    "        cost_bps (float): Transaction cost in basis points for backtest.\n",
    "\n",
    "    Returns:\n",
    "        metrics (Dict[str, float]): Dictionary with evaluation metrics.\n",
    "        Optionally:\n",
    "            y_pred (np.ndarray): Predicted values, shape (N,).\n",
    "            y_true (np.ndarray): Ground-truth values, shape (N,).\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            try:\n",
    "                # Use dict keys from safe_collate\n",
    "                x, y_true = batch[\"input\"].to(device), batch[\"target\"].to(device).unsqueeze(1)\n",
    "\n",
    "                y_pred = model(x).detach().cpu().numpy()\n",
    "                all_preds.append(y_pred)\n",
    "                all_targets.append(y_true.cpu().numpy())\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"âš ï¸ Skipped batch during evaluation: {e}\")\n",
    "                continue\n",
    "\n",
    "    if not all_preds or not all_targets:\n",
    "        raise RuntimeError(\"âŒ No valid predictions. Check dataloader or model output.\")\n",
    "\n",
    "    # Flatten arrays\n",
    "    y_true = np.concatenate(all_targets).ravel()\n",
    "    y_pred = np.concatenate(all_preds).ravel()\n",
    "\n",
    "    # --- Statistical Metrics ---\n",
    "    non_zero = y_true != 0\n",
    "    if np.any(non_zero):\n",
    "        mape = np.mean(np.abs((y_true[non_zero] - y_pred[non_zero]) / y_true[non_zero])) * 100\n",
    "    else:\n",
    "        mape = np.nan\n",
    "\n",
    "    metrics = {\n",
    "        \"RMSE\": round(np.sqrt(mean_squared_error(y_true, y_pred)), 4),\n",
    "        \"MAE\": round(mean_absolute_error(y_true, y_pred), 4),\n",
    "        \"MAPE\": round(mape, 2),\n",
    "        \"R2\": round(r2_score(y_true, y_pred), 4),\n",
    "    }\n",
    "\n",
    "    # --- Financial Backtest Metrics ---\n",
    "    try:\n",
    "        bt_metrics = backtest_strategy(y_pred, y_true, cost_bps=cost_bps)\n",
    "        metrics.update({\n",
    "            \"AnnReturn\": round(bt_metrics[\"Annualized Return\"], 4),\n",
    "            \"AnnVol\": round(bt_metrics[\"Annualized Volatility\"], 4),\n",
    "            \"Sharpe\": round(bt_metrics[\"Sharpe\"], 4),\n",
    "            \"MaxDD\": round(bt_metrics[\"Max Drawdown\"], 4),\n",
    "            \"WinRate\": round(bt_metrics[\"Win Rate\"], 4),\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"âš ï¸ Backtest failed: {e}\")\n",
    "\n",
    "    # --- Logging summary ---\n",
    "    logger.info(\n",
    "        f\"ðŸ“Š Evaluation | \"\n",
    "        f\"RMSE: {metrics['RMSE']:.4f} | \"\n",
    "        f\"MAE: {metrics['MAE']:.4f} | \"\n",
    "        f\"MAPE: {metrics['MAPE']:.2f}% | \"\n",
    "        f\"RÂ²: {metrics['R2']:.4f} | \"\n",
    "        f\"Sharpe: {metrics.get('Sharpe', np.nan):.4f} | \"\n",
    "        f\"MaxDD: {metrics.get('MaxDD', np.nan):.4f} | \"\n",
    "        f\"WinRate: {metrics.get('WinRate', np.nan):.2f}\"\n",
    "    )\n",
    "\n",
    "    return (metrics, y_pred, y_true) if return_predictions else metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4be90-f09d-4e56-9f10-57ac3db3375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# GADHT - Model Saving Utility\n",
    "# ===========================================================\n",
    "\n",
    "def save_gadht_model(\n",
    "    model: nn.Module,\n",
    "    filename: str = \"gadht_pretrained.pt\",\n",
    "    directory: str = \"checkpoints/pretraining\",\n",
    "    hyperparams: Optional[dict] = None,\n",
    "    save_full_model: bool = False\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Save the state_dict of a GADHT model after pretraining or fine-tuning.\n",
    "    Optionally also saves hyperparameters and/or the full model object.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): GADHT model (or wrapper).\n",
    "        filename (str): Target filename for the saved model.\n",
    "        directory (str): Directory where the model will be stored.\n",
    "        hyperparams (dict, optional): Dictionary of model hyperparameters to save alongside weights.\n",
    "        save_full_model (bool): If True, save the full model object (not recommended for production).\n",
    "\n",
    "    Returns:\n",
    "        str or None: Full path of the saved model if successful, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        save_path = os.path.join(directory, filename)\n",
    "\n",
    "        if save_full_model:\n",
    "            # Warning: less portable, but keeps full class structure\n",
    "            torch.save(model, save_path)\n",
    "            logger.info(f\"âœ… Full model object saved â†’ {save_path}\")\n",
    "        else:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            logger.info(f\"âœ… Model state_dict saved â†’ {save_path}\")\n",
    "\n",
    "        # Save hyperparameters if provided\n",
    "        if hyperparams is not None:\n",
    "            hp_path = os.path.splitext(save_path)[0] + \"_hparams.json\"\n",
    "            with open(hp_path, \"w\") as f:\n",
    "                json.dump(hyperparams, f, indent=4)\n",
    "            logger.info(f\"âš™ï¸  Hyperparameters saved â†’ {hp_path}\")\n",
    "\n",
    "        return save_path\n",
    "\n",
    "    except (OSError, IOError) as e:\n",
    "        logger.error(f\"âŒ File system error while saving model: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Unexpected error while saving model: {e}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c03bc6-cc39-4248-ad23-2f0a1d632a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# GADHT - Model Loading Utility\n",
    "# ===========================================================\n",
    "\n",
    "def load_gadht_model(\n",
    "    model: Optional[nn.Module],\n",
    "    filename: str,\n",
    "    directory: str = \"checkpoints/pretraining\",\n",
    "    map_location: Optional[str] = None,\n",
    "    strict: bool = True,\n",
    "    load_hparams: bool = True\n",
    ") -> Union[nn.Module, Tuple[nn.Module, dict]]:\n",
    "    \"\"\"\n",
    "    Load model weights (and optionally hyperparameters) into an initialized GADHT architecture.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module or None): Initialized GADHT model. If None, will try to\n",
    "                                   rebuild from saved hyperparameters.\n",
    "        filename (str): Name of the saved weight file (.pt or .pth).\n",
    "        directory (str): Folder where the model checkpoint is stored.\n",
    "        map_location (Optional[str]): Device override ('cpu', 'cuda', or None).\n",
    "        strict (bool): Whether to strictly enforce state_dict key matching.\n",
    "        load_hparams (bool): If True, also try to load '<filename>_hparams.json'.\n",
    "    \n",
    "    Returns:\n",
    "        model (nn.Module) or (model, hparams) if load_hparams=True and file exists.\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If the checkpoint file does not exist.\n",
    "        RuntimeError: If state_dict loading fails due to mismatch (when strict=True).\n",
    "        Exception: For unexpected errors during loading.\n",
    "    \"\"\"\n",
    "    load_path = os.path.join(directory, filename)\n",
    "    try:\n",
    "        location = map_location or torch.device(\"cpu\")\n",
    "        state_dict = torch.load(load_path, map_location=location)\n",
    "\n",
    "        # Load hyperparameters if available\n",
    "        hparams = None\n",
    "        if load_hparams:\n",
    "            hp_path = os.path.splitext(load_path)[0] + \"_hparams.json\"\n",
    "            if os.path.exists(hp_path):\n",
    "                with open(hp_path, \"r\") as f:\n",
    "                    hparams = json.load(f)\n",
    "                logger.info(f\"âš™ï¸ Loaded hyperparameters from {hp_path}\")\n",
    "                # If model not provided, try to re-init automatically\n",
    "                if model is None:\n",
    "                    model = GADHTModel(**hparams)\n",
    "                    logger.info(\"ðŸ› ï¸ Reconstructed GADHTModel from hyperparameters.\")\n",
    "\n",
    "        if model is None:\n",
    "            raise ValueError(\"Model is None and no hyperparameters were found to rebuild it.\")\n",
    "\n",
    "        model.load_state_dict(state_dict, strict=strict)\n",
    "        logger.info(f\"âœ… Model weights loaded successfully from: {load_path}\")\n",
    "\n",
    "        return (model, hparams) if hparams is not None else model\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"âŒ Model file not found: {load_path}\")\n",
    "        raise\n",
    "    except RuntimeError as e:\n",
    "        logger.error(f\"âŒ State_dict mismatch while loading model (strict={strict}): {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Unexpected error while loading model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df69d3b-fd25-4164-a241-94cb7d781f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# GADHT - Full Pretraining Pipeline (Tickers-Aware)\n",
    "# ===========================================================\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸ”§ Pretraining configuration\n",
    "# ----------------------------\n",
    "PRETRAIN_EPOCHS = 50\n",
    "PRETRAIN_BATCH_SIZE = 64\n",
    "PRETRAIN_LR = 1e-5\n",
    "PRETRAIN_PATIENCE = 8\n",
    "\n",
    "CHECKPOINT_DIR = \"checkpoints/pretraining\"\n",
    "PLOT_DIR = \"results/pretraining\"\n",
    "PRETRAINED_MODEL_FILE = \"gadht_pretrained.pt\"\n",
    "PRETRAIN_LOSS_CURVE_FILE = \"pretraining_loss_curve.png\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1 â€” Load dataset\n",
    "# ----------------------------\n",
    "pretrain_tickers = PHASE_TICKERS.get(\"pretraining\", [])\n",
    "if not pretrain_tickers:\n",
    "    logger.critical(\"âŒ No tickers defined for pretraining phase.\")\n",
    "    raise RuntimeError(\"No pretraining tickers available.\")\n",
    "\n",
    "logger.info(f\"ðŸ” Loading pretraining dataset for tickers: {pretrain_tickers}\")\n",
    "\n",
    "try:\n",
    "    pretrain_dataset = load_ticker_datasets(\n",
    "        tickers=pretrain_tickers,\n",
    "        data_path=DATA_DIRECTORY,\n",
    "        window_size=30,\n",
    "        max_imfs=5,\n",
    "        use_ceemdan=True,\n",
    "        pretraining_mode=True,   # IMF masking for self-supervised training\n",
    "        normalize=True,\n",
    "        prediction_horizon=1     # always 1-day ahead for pretraining\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"âŒ Failed to load pretraining dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2 â€” DataLoader setup\n",
    "# ----------------------------\n",
    "pretrain_loader = DataLoader(\n",
    "    pretrain_dataset,\n",
    "    batch_size=PRETRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=safe_collate\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"ðŸ“¦ Pretraining dataset ready \"\n",
    "    f\"({len(pretrain_dataset)} samples, batch_size={PRETRAIN_BATCH_SIZE})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2caabf-8ced-453d-b27e-a40ed117fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Step 2 â€” ðŸ§  Initialize model (with wrapper for IMF reconstruction)\n",
    "# ===========================================================\n",
    "\n",
    "logger.info(\"ðŸ§  Initializing GADHT base model and pretraining wrapper...\")\n",
    "\n",
    "try:\n",
    "    # Base GADHT model in pretraining mode (outputs reconstructed (T, M, F) sequences)\n",
    "    base_model = GADHTModel(pretraining=True)\n",
    "    \n",
    "    # Wrapper ensures correct interface for masked IMF reconstruction training\n",
    "    pretrain_model = GADHTPretrainingWrapper(base_model)\n",
    "\n",
    "    logger.info(\n",
    "        f\"âœ… Model initialized | \"\n",
    "        f\"d_model={base_model.d_model}, \"\n",
    "        f\"IMFs={base_model.num_imfs}, \"\n",
    "        f\"features={base_model.num_features}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.critical(f\"âŒ Failed to initialize GADHT pretraining model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1168698a-c84e-496e-9a3e-a28eb75241df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Step 3 â€” ðŸ‹ï¸ Launch self-supervised pretraining\n",
    "# ===========================================================\n",
    "\n",
    "logger.info(\"ðŸ Starting GADHT self-supervised pretraining...\")\n",
    "\n",
    "try:\n",
    "    run_gadht_pretraining(\n",
    "        model=pretrain_model,\n",
    "        dataloader=pretrain_loader,\n",
    "        num_epochs=PRETRAIN_EPOCHS,\n",
    "        learning_rate=PRETRAIN_LR,\n",
    "        early_stopping_patience=PRETRAIN_PATIENCE,\n",
    "        max_gradient_norm=1.0,\n",
    "        checkpoint_path=os.path.join(CHECKPOINT_DIR, PRETRAINED_MODEL_FILE),\n",
    "        plot_loss_curve=True,\n",
    "        loss_plot_path=os.path.join(PLOT_DIR, PRETRAIN_LOSS_CURVE_FILE)\n",
    "    )\n",
    "    logger.info(f\"âœ… Pretraining completed successfully. Model saved to {os.path.join(CHECKPOINT_DIR, PRETRAINED_MODEL_FILE)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.critical(f\"âŒ Pretraining failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a589f6d-5068-41e6-808b-7204cfc52fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Step 4 â€” ðŸ’¾ Save pretrained encoder weights\n",
    "# ===========================================================\n",
    "\n",
    "logger.info(\"ðŸ’¾ Saving pretrained encoder weights...\")\n",
    "\n",
    "try:\n",
    "    saved_path = save_gadht_model(\n",
    "        model=base_model,  # only encoder, not wrapper\n",
    "        filename=PRETRAINED_MODEL_FILE,\n",
    "        directory=CHECKPOINT_DIR\n",
    "    )\n",
    "\n",
    "    if saved_path:\n",
    "        logger.info(f\"âœ… Pretrained encoder weights saved at {saved_path}\")\n",
    "    else:\n",
    "        logger.warning(\"âš ï¸ Encoder weights could not be saved.\")\n",
    "\n",
    "    logger.info(\"ðŸŽ¯ GADHT pretraining completed. Encoder and loss curve saved.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.critical(f\"âŒ Failed to save pretrained encoder weights: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d7a9c-5b96-424b-9fb6-82a0347ad64b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# GADHT - Fine-Tuning via Rolling CV + Final Model Training + Backtest\n",
    "# ===========================================================\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸ”§ Fine-tuning configuration\n",
    "# ----------------------------\n",
    "FINETUNE_EPOCHS = 150\n",
    "FINETUNE_BATCH_SIZE = 64\n",
    "FINETUNE_LR = 1e-5\n",
    "FINETUNE_PATIENCE = 10\n",
    "\n",
    "PRETRAINED_MODEL_PATH = \"checkpoints/pretraining/gadht_pretrained.pt\"\n",
    "FINETUNE_MODEL_DIR = \"checkpoints/finetuned\"\n",
    "RESULTS_DIR = \"results/finetune\"\n",
    "os.makedirs(FINETUNE_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Assets to fine-tune on (single + multi-asset)\n",
    "# ----------------------------\n",
    "FINE_TUNE_TICKER_GROUPS = {\n",
    "    \"tsla\": [\"TSLA\"],\n",
    "    \"jnj\": [\"JNJ\"],\n",
    "    \"msft\": [\"MSFT\"],\n",
    "    \"nke\": [\"NKE\"],\n",
    "    \"pg\":  [\"PG\"],\n",
    "    \"unh\": [\"UNH\"],\n",
    "    \"all_assets\": [\"TSLA\", \"JNJ\", \"MSFT\", \"NKE\", \"PG\", \"UNH\"]\n",
    "}\n",
    "\n",
    "HORIZONS = [1, 5, 10]  # run fine-tuning for 1, 5, and 10 days ahead\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main fine-tuning loop\n",
    "# ----------------------------\n",
    "for model_name, tickers in FINE_TUNE_TICKER_GROUPS.items():\n",
    "    for H in HORIZONS:\n",
    "        try:\n",
    "            logger.info(f\"ðŸš€ Running 5-fold CV for: {model_name.upper()} | Horizon={H}\")\n",
    "\n",
    "            # === 1. Rolling-origin CV (includes backtest inside evaluate_model)\n",
    "            run_gadht_rolling_cv(\n",
    "                tickers=tickers,\n",
    "                model_name=f\"finetuned_{model_name}_H{H}\",\n",
    "                dataset_name=\" + \".join(tickers),\n",
    "                pretrained_path=PRETRAINED_MODEL_PATH,\n",
    "                n_splits=5,\n",
    "                batch_size=FINETUNE_BATCH_SIZE,\n",
    "                lr=FINETUNE_LR,\n",
    "                epochs=FINETUNE_EPOCHS,\n",
    "                patience=FINETUNE_PATIENCE,\n",
    "                max_grad_norm=1.0,\n",
    "                prediction_horizon=H\n",
    "            )\n",
    "\n",
    "            # === 2. Final training on full dataset\n",
    "            logger.info(f\"ðŸ§  Training final model on full dataset: {model_name.upper()} | Horizon={H}\")\n",
    "\n",
    "            full_dataset = load_ticker_datasets(\n",
    "                tickers=tickers,\n",
    "                data_path=DATA_DIRECTORY,\n",
    "                window_size=30,\n",
    "                max_imfs=5,\n",
    "                use_ceemdan=True,\n",
    "                pretraining_mode=False,\n",
    "                normalize=True,\n",
    "                prediction_horizon=H\n",
    "            )\n",
    "\n",
    "            full_loader = DataLoader(\n",
    "                full_dataset,\n",
    "                batch_size=FINETUNE_BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                collate_fn=safe_collate\n",
    "            )\n",
    "\n",
    "            # Load pretrained encoder\n",
    "            final_model = GADHTModel(pretraining=False).to(device)\n",
    "            final_model.load_state_dict(\n",
    "                torch.load(PRETRAINED_MODEL_PATH, map_location=device),\n",
    "                strict=False\n",
    "            )\n",
    "\n",
    "            save_path = os.path.join(FINETUNE_MODEL_DIR, f\"gadht_finetuned_{model_name}_H{H}.pt\")\n",
    "\n",
    "            run_gadht_finetune(\n",
    "                model=final_model,\n",
    "                dataloader=full_loader,\n",
    "                epochs=FINETUNE_EPOCHS,\n",
    "                lr=FINETUNE_LR,\n",
    "                patience=FINETUNE_PATIENCE,\n",
    "                max_grad_norm=1.0,\n",
    "                save_path=save_path,\n",
    "                plot_curve=True,\n",
    "                fig_path=os.path.join(RESULTS_DIR, f\"{model_name}_final_curve_H{H}.png\")\n",
    "            )\n",
    "\n",
    "            logger.info(f\"âœ… Final fine-tuned model saved: {save_path}\")\n",
    "\n",
    "            # === 3. Backtest on full dataset\n",
    "            logger.info(f\"ðŸ“Š Running backtest on full dataset: {model_name.upper()} | Horizon={H}\")\n",
    "            metrics, preds, targets = evaluate_model(\n",
    "                final_model,\n",
    "                DataLoader(full_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=False, collate_fn=safe_collate),\n",
    "                return_predictions=True,\n",
    "                device=device,\n",
    "                cost_bps=20  # 20 bps transaction cost\n",
    "            )\n",
    "\n",
    "            results_path = os.path.join(RESULTS_DIR, f\"{model_name}_final_backtest_H{H}.csv\")\n",
    "            pd.DataFrame([metrics]).to_csv(results_path, index=False)\n",
    "            logger.info(f\"âœ… Backtest results saved: {results_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"âŒ Fine-tuning failed for {model_name.upper()} | Horizon={H}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1cf034-3ea6-4e65-936f-4266bd5472cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# âœ… ZEROSHOT PIPELINE â€“ All Zero-Shot Assets (with Backtesting)\n",
    "# ===========================================================\n",
    "\n",
    "def backtest_portfolio(preds, targets, cost=0.002, risk_free_rate=0.0, freq=252):\n",
    "    \"\"\"\n",
    "    Run a simple backtest given predictions and true values.\n",
    "    \"\"\"\n",
    "    preds = np.array(preds)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    if len(preds) != len(targets) or len(preds) < 2:\n",
    "        return {\"Sharpe\": 0.0, \"MaxDD\": 0.0, \"WinRate\": 0.0, \"EquityCurve\": np.array([])}\n",
    "\n",
    "    # Trading signals\n",
    "    signals = np.sign(np.diff(preds, prepend=preds[0]))\n",
    "\n",
    "    # Realized returns\n",
    "    rets = np.diff(targets) / targets[:-1]\n",
    "    rets = np.concatenate([[0], rets])\n",
    "\n",
    "    # Strategy returns\n",
    "    strat_rets = signals * rets\n",
    "\n",
    "    # Transaction costs\n",
    "    trades = np.diff(signals) != 0\n",
    "    trades = np.concatenate([[False], trades])\n",
    "    strat_rets[trades] -= cost\n",
    "\n",
    "    # Equity curve\n",
    "    equity = np.cumprod(1 + strat_rets)\n",
    "\n",
    "    # Sharpe ratio\n",
    "    mu, sigma = strat_rets.mean(), strat_rets.std() + 1e-8\n",
    "    sharpe = (mu - risk_free_rate / freq) / sigma * np.sqrt(freq)\n",
    "\n",
    "    # Max drawdown\n",
    "    peak = np.maximum.accumulate(equity)\n",
    "    drawdown = equity / peak - 1\n",
    "    max_dd = drawdown.min()\n",
    "\n",
    "    # Win rate\n",
    "    win_rate = (strat_rets > 0).mean()\n",
    "\n",
    "    return {\n",
    "        \"Sharpe\": float(sharpe),\n",
    "        \"MaxDD\": float(max_dd),\n",
    "        \"WinRate\": float(win_rate),\n",
    "        \"EquityCurve\": equity\n",
    "    }\n",
    "\n",
    "\n",
    "def run_zero_shot_evaluation(model_path, save_dir=\"figures/zeroshot\"):\n",
    "    \"\"\"\n",
    "    Run zero-shot evaluation on all assets defined in PHASE_TICKERS[\"zeroshot\"].\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    results = []\n",
    "\n",
    "    tickers = PHASE_TICKERS[\"zeroshot\"]\n",
    "\n",
    "    for ticker in tickers:\n",
    "        logger.info(f\"\\nðŸ§ª Zero-Shot Evaluation on {ticker}...\")\n",
    "\n",
    "        # === Load dataset\n",
    "        dataset = load_ticker_datasets(\n",
    "            tickers=[ticker],\n",
    "            data_path=DATA_DIRECTORY,\n",
    "            window_size=30,\n",
    "            max_imfs=5,\n",
    "            use_ceemdan=True,\n",
    "            pretraining_mode=False,\n",
    "            normalize=True,\n",
    "            prediction_horizon=1\n",
    "        )\n",
    "        logger.info(f\"ðŸŒŸ Dataset ready for {ticker}: {len(dataset)} samples.\")\n",
    "\n",
    "        # === 5-fold rolling validation\n",
    "        kf = KFold(n_splits=5, shuffle=False)\n",
    "        all_preds, all_targets = [], []\n",
    "        fold_metrics = []\n",
    "\n",
    "        for fold, (_, val_idx) in enumerate(kf.split(dataset)):\n",
    "            val_subset = Subset(dataset, val_idx)\n",
    "            val_loader = DataLoader(\n",
    "                val_subset,\n",
    "                batch_size=64,\n",
    "                shuffle=False,\n",
    "                collate_fn=safe_collate\n",
    "            )\n",
    "\n",
    "            # Load pretrained model\n",
    "            model = GADHTModel(pretraining=False).to(device)\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "            model.eval()\n",
    "\n",
    "            preds, targets = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    if batch is None:\n",
    "                        continue\n",
    "\n",
    "                    # Handle dict batches (from safe_collate)\n",
    "                    if isinstance(batch, dict):\n",
    "                        if \"input\" in batch and \"target\" in batch:  # supervised mode\n",
    "                            x, y = batch[\"input\"], batch[\"target\"]\n",
    "                        elif \"masked\" in batch:  # pretraining mode (should not happen here)\n",
    "                            logger.warning(\"âš ï¸ Skipping pretraining batch in zero-shot eval.\")\n",
    "                            continue\n",
    "                        else:\n",
    "                            logger.error(f\"âŒ Unexpected batch keys: {list(batch.keys())}\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        try:\n",
    "                            x, y = batch\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"âŒ Could not unpack batch: {e}\")\n",
    "                            continue\n",
    "\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    y_hat = model(x).squeeze()\n",
    "                    preds.append(y_hat.cpu().numpy())\n",
    "                    targets.append(y.cpu().numpy())\n",
    "\n",
    "            if not preds or not targets:\n",
    "                logger.warning(f\"âš ï¸ No valid predictions for fold {fold+1} of {ticker}, skipping...\")\n",
    "                continue\n",
    "\n",
    "            preds = np.concatenate(preds)\n",
    "            targets = np.concatenate(targets)\n",
    "\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(targets)\n",
    "\n",
    "            # Fold metrics\n",
    "            rmse = np.sqrt(mean_squared_error(targets, preds))\n",
    "            mae = mean_absolute_error(targets, preds)\n",
    "            mape = np.mean(np.abs((targets - preds) / targets)) * 100\n",
    "            r2 = r2_score(targets, preds)\n",
    "\n",
    "            fold_metrics.append({\"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape, \"R2\": r2})\n",
    "\n",
    "        if not fold_metrics:\n",
    "            logger.warning(f\"âŒ No valid folds for {ticker}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # === Aggregate fold metrics\n",
    "        avg_metrics = pd.DataFrame(fold_metrics).mean().to_dict()\n",
    "\n",
    "        # === Concatenate predictions\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "\n",
    "        # Backtest\n",
    "        bt_metrics = backtest_portfolio(all_preds, all_targets, cost=0.002)\n",
    "\n",
    "        # Save metrics\n",
    "        results.append({\"Stock\": ticker, **avg_metrics, **bt_metrics})\n",
    "\n",
    "        # === Save plots\n",
    "        plot_predictions_vs_real_array(\n",
    "            all_targets, all_preds, ticker,\n",
    "            strategy=\"Zero-shot\",\n",
    "            save_path=f\"{save_dir}/{ticker.lower()}_pred_actual.png\"\n",
    "        )\n",
    "        plot_scatter_array(\n",
    "            all_targets, all_preds, ticker,\n",
    "            strategy=\"Zero-shot\",\n",
    "            save_path=f\"{save_dir}/{ticker.lower()}_scatter.png\"\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d6b62-1096-413a-a9d8-48f8dd8bb239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# ðŸ“ˆ ZEROSHOT VISUALIZATIONS\n",
    "# ===========================================================\n",
    "\n",
    "def plot_predictions_vs_real_array(y_true, y_pred, stock_name, strategy=\"Zero-shot\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plot time-series comparison between actual and predicted values.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Ground truth values.\n",
    "        y_pred (array-like): Model predictions.\n",
    "        stock_name (str): Ticker symbol (e.g., \"KO\", \"META\").\n",
    "        strategy (str): Label for strategy (default = \"Zero-shot\").\n",
    "        save_path (str or None): Path to save figure (if provided).\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_true, label=\"Actual\", linestyle=\"--\", linewidth=2, color=\"black\")\n",
    "    plt.plot(y_pred, label=\"Prediction\", linewidth=2, color=\"royalblue\", alpha=0.8)\n",
    "    plt.title(f\"{stock_name} â€“ Close Price Prediction (+1 day) [{strategy}]\", fontsize=16)\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Normalized Price\")\n",
    "    plt.legend(framealpha=0.8)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_scatter_array(y_true, y_pred, stock_name, strategy=\"Zero-shot\", save_path=None):\n",
    "    \"\"\"\n",
    "    Scatter plot of actual vs predicted values, with diagonal reference.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Ground truth values.\n",
    "        y_pred (array-like): Model predictions.\n",
    "        stock_name (str): Ticker symbol (e.g., \"KO\", \"META\").\n",
    "        strategy (str): Label for strategy (default = \"Zero-shot\").\n",
    "        save_path (str or None): Path to save figure (if provided).\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Compute RÂ² for quick visual feedback\n",
    "    try:\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "    except Exception:\n",
    "        r2 = np.nan\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6, color=\"dodgerblue\", edgecolor=\"k\")\n",
    "    min_val, max_val = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label=\"Ideal Fit\")\n",
    "    plt.title(f\"{stock_name} â€“ Actual vs Predicted [{strategy}] (RÂ²={r2:.3f})\", fontsize=16)\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.legend(framealpha=0.8)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfab221-f0ab-4641-aee8-f2933d97fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# âš¡ï¸ Execution du pipeline zero-shot\n",
    "# ========================================\n",
    "\n",
    "PRETRAINED_MODEL_PATH = \"checkpoints/finetuned/gadht_finetuned_all_assets_H1.pt\"\n",
    "\n",
    "zeroshot_df = run_zero_shot_evaluation(\n",
    "    model_path=PRETRAINED_MODEL_PATH\n",
    ")\n",
    "\n",
    "print(zeroshot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242873c-5b71-4204-b6eb-9edd2edb43fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage console\n",
    "print(zeroshot_df.round(4))\n",
    "\n",
    "os.makedirs(\"results/zeroshot\", exist_ok=True)\n",
    "\n",
    "# Export CSV + LaTeX\n",
    "zeroshot_df.round(2).to_csv(\"results/zeroshot/zeroshot_summary.csv\", index=False)\n",
    "print(zeroshot_df.round(2).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75737a6-94d9-410e-b925-1a07209d5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# ðŸ“Š IMF & Temporal Attention Analysis (All Fine-Tuning Assets)\n",
    "# ===========================================================\n",
    "\n",
    "# Make sure these come from the main codebase\n",
    "# from model import GADHTModel\n",
    "# from data import load_ticker_datasets, safe_collate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define all fine-tuning checkpoints (1-day horizon models)\n",
    "CHECKPOINTS = {\n",
    "    \"TSLA\": \"checkpoints/finetuned/gadht_finetuned_tsla_H1.pt\",\n",
    "    \"JNJ\":  \"checkpoints/finetuned/gadht_finetuned_jnj_H1.pt\",\n",
    "    \"MSFT\": \"checkpoints/finetuned/gadht_finetuned_msft_H1.pt\",\n",
    "    \"NKE\":  \"checkpoints/finetuned/gadht_finetuned_nke_H1.pt\",\n",
    "    \"PG\":   \"checkpoints/finetuned/gadht_finetuned_pg_H1.pt\",\n",
    "    \"UNH\":  \"checkpoints/finetuned/gadht_finetuned_unh_H1.pt\",\n",
    "}\n",
    "\n",
    "FIG_INT_DIR = \"figures/interpretability\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_IMFS = 5\n",
    "\n",
    "# Ensure figure directory exists\n",
    "os.makedirs(FIG_INT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d29fc-17c6-4169-bed4-eeb1fadc5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# ðŸ” Extract Average Temporal Attention Weights\n",
    "# ===========================================================\n",
    "\n",
    "def extract_average_temporal_attention(ticker, model_path, max_batches=30):\n",
    "    \"\"\"\n",
    "    Extract the average temporal attention weights for a given ticker\n",
    "    using the first temporal attention layer of GADHT.\n",
    "    \"\"\"\n",
    "    model = GADHTModel(pretraining=False).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    dataset = load_ticker_datasets(\n",
    "        [ticker],\n",
    "        data_path=DATA_DIRECTORY,\n",
    "        window_size=30,\n",
    "        max_imfs=5,\n",
    "        use_ceemdan=True,\n",
    "        pretraining_mode=False,\n",
    "        normalize=True,\n",
    "        prediction_horizon=1\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=safe_collate)\n",
    "\n",
    "    all_weights = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "\n",
    "            # Supervised mode â†’ dict {\"input\", \"target\"}\n",
    "            if isinstance(batch, dict):\n",
    "                x = batch[\"input\"].to(device)   # (B, T, M, F)\n",
    "            else:\n",
    "                x, _ = batch\n",
    "                x = x.to(device)\n",
    "\n",
    "            B, T, M, F = x.shape\n",
    "\n",
    "            # Reshape: treat each IMF as an independent sequence\n",
    "            x_tm = x.permute(0, 2, 1, 3).contiguous().view(B * M, T, F)  # (B*M, T, F)\n",
    "            x_tm = model.input_proj(x_tm)  # (B*M, T, d_model)\n",
    "\n",
    "            # Add positional encoding\n",
    "            x_tm = x_tm + model.positional_encoding.to(x_tm.device)\n",
    "\n",
    "            # Take the FIRST temporal attention layer for interpretability\n",
    "            x_tm, attn = model.temporal_layers[0](x_tm, energy=None)  # attn: (B*M, heads, T, T)\n",
    "\n",
    "            # Average over heads and IMF dimension\n",
    "            attn_mean = attn.mean(dim=1)         # (B*M, T, T)\n",
    "            attn_mean = attn_mean.mean(dim=0)    # (T, T)\n",
    "            all_weights.append(attn_mean.cpu().numpy())\n",
    "\n",
    "    if not all_weights:\n",
    "        logger.warning(f\"âš ï¸ No attention extracted for {ticker}\")\n",
    "        return np.array([])\n",
    "\n",
    "    # Average across batches\n",
    "    mean_attn_matrix = np.mean(np.stack(all_weights, axis=0), axis=0)  # (T, T)\n",
    "\n",
    "    # Collapse query dimension â†’ importance per time step\n",
    "    mean_weights = mean_attn_matrix.mean(axis=0)  # (T,)\n",
    "\n",
    "    return mean_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef5994-7ed8-4093-8487-65b1a80724b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# ðŸ“ˆ Temporal Attention Weights â€“ All Fine-Tuning Assets\n",
    "# ===========================================================\n",
    "\n",
    "weights_dict = {}\n",
    "for ticker, path in CHECKPOINTS.items():\n",
    "    weights = extract_average_temporal_attention(ticker, path)\n",
    "    if weights.size == 0:\n",
    "        logger.warning(f\"âš ï¸ Skipping {ticker}, no valid attention weights.\")\n",
    "        continue\n",
    "    weights_dict[ticker] = weights\n",
    "\n",
    "if not weights_dict:\n",
    "    raise RuntimeError(\"âŒ No attention weights available to plot.\")\n",
    "\n",
    "# âœ… Build time step labels dynamically\n",
    "time_labels = [f\"T{i+1}\" for i in range(len(next(iter(weights_dict.values()))))]\n",
    "\n",
    "# Plot all assets\n",
    "plt.figure(figsize=(16, 8))  # Large canvas for readability\n",
    "\n",
    "for ticker, weights in weights_dict.items():\n",
    "    plt.plot(time_labels, weights, marker=\"o\", linewidth=2, label=ticker)\n",
    "\n",
    "plt.title(\"Mean Attention Weights per Time Step (Fine-Tuning Assets)\", fontsize=18)\n",
    "plt.ylabel(\"Average Attention Weight\", fontsize=14)\n",
    "plt.xlabel(\"Time Step\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend(fontsize=12, ncol=2)  # multi-column legend for clarity\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_INT_DIR}/temporal_attention_weights_all.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c165d-3bbe-4e54-9d1d-e2844993eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# ðŸ” Temporal Attention Heatmap for 1 Sample\n",
    "# ===========================================================\n",
    "\n",
    "def plot_temporal_attention_map(ticker, model_path):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of temporal attention weights for a single sample.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): Stock ticker (e.g., \"TSLA\").\n",
    "        model_path (str): Path to the fine-tuned checkpoint.\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = GADHTModel(pretraining=False).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_ticker_datasets(\n",
    "        [ticker],\n",
    "        data_path=DATA_DIRECTORY,\n",
    "        window_size=30,\n",
    "        max_imfs=5,\n",
    "        use_ceemdan=True,\n",
    "        pretraining_mode=False,\n",
    "        normalize=True,\n",
    "        prediction_horizon=1\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=safe_collate)\n",
    "\n",
    "    attn_map = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            x = batch[\"input\"].to(device)   # (1, T, M, F)\n",
    "\n",
    "            B, T, M, F = x.shape\n",
    "            # Prepare temporal input: (B*M, T, F)\n",
    "            x_tm = x.permute(0, 2, 1, 3).contiguous().view(B * M, T, F)\n",
    "            x_tm = model.input_proj(x_tm) + model.positional_encoding.to(x_tm.device)\n",
    "\n",
    "            # Run through first temporal attention layer to extract attention map\n",
    "            x_tm, attn = model.temporal_layers[0](x_tm, energy=None)  # attn: (heads, T, T) or (B*M, heads, T, T)\n",
    "\n",
    "            # Average over heads and batch dimension if necessary\n",
    "            if attn.dim() == 4:\n",
    "                attn_map = attn.mean(dim=1)[0].cpu().numpy()  # (T, T) from first IMF\n",
    "            elif attn.dim() == 3:\n",
    "                attn_map = attn.mean(dim=0).cpu().numpy()     # (T, T)\n",
    "            break\n",
    "\n",
    "    if attn_map is None:\n",
    "        logger.warning(f\"âš ï¸ No attention map extracted for {ticker}\")\n",
    "        return\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(attn_map, cmap=\"YlGnBu\", xticklabels=False, yticklabels=False)\n",
    "    plt.title(f\"Temporal Attention Map â€“ {ticker} (1-day horizon)\")\n",
    "    plt.xlabel(\"Time (Key)\")\n",
    "    plt.ylabel(\"Time (Query)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FIG_INT_DIR}/{ticker.lower()}_temporal_attention_map_H1.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e9e11-601a-4874-ba24-7ab783e0bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# ðŸ” Temporal Attention Maps â€“ All Fine-Tuning Assets\n",
    "# ===========================================================\n",
    "\n",
    "for ticker, path in CHECKPOINTS.items():\n",
    "    logger.info(f\"ðŸ–¼ï¸ Generating temporal attention map for {ticker}...\")\n",
    "    plot_temporal_attention_map(ticker, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3515b-9b72-41fc-b091-dff9538649fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# ðŸ“ˆ IMF-Level Attention Analysis (L2-Norm Method)\n",
    "# ===========================================================\n",
    "\n",
    "def analyze_imf_attention_weights(ticker, model_path):\n",
    "    \"\"\"\n",
    "    Compute relative IMF-level importance weights using L2-norm energy.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): Stock ticker (e.g., \"TSLA\").\n",
    "        model_path (str): Path to fine-tuned checkpoint.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized IMF importance weights (shape: [num_imfs]).\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = GADHTModel(pretraining=False).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_ticker_datasets(\n",
    "        [ticker],\n",
    "        data_path=DATA_DIRECTORY,\n",
    "        window_size=30,\n",
    "        max_imfs=5,\n",
    "        use_ceemdan=True,\n",
    "        pretraining_mode=False,\n",
    "        normalize=True,\n",
    "        prediction_horizon=1\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=safe_collate)\n",
    "\n",
    "    all_weights = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            # Extract input tensor\n",
    "            X = batch[\"input\"].to(device)  # (B, T, M, F)\n",
    "            B, T, M, F = X.shape\n",
    "\n",
    "            # Compute IMF-level L2 energy across time and features\n",
    "            imf_energies = []\n",
    "            for i in range(M):\n",
    "                imf_energy = X[:, :, i, :].norm(p=2, dim=-1).mean().item()\n",
    "                imf_energies.append(imf_energy)\n",
    "\n",
    "            # Normalize to obtain importance weights\n",
    "            imf_weights = np.array(imf_energies)\n",
    "            imf_weights /= imf_weights.sum()\n",
    "\n",
    "            all_weights.append(imf_weights)\n",
    "\n",
    "    # Return average across samples\n",
    "    return np.mean(all_weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b079f-7151-455b-bfdd-0f1c7fc287ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# ðŸ“Š Aggregated Bar Plot for Multiple Stocks (IMF Weights)\n",
    "# ===========================================================\n",
    "\n",
    "results = {}\n",
    "for ticker in CHECKPOINTS:\n",
    "    weights = analyze_imf_attention_weights(ticker, CHECKPOINTS[ticker])\n",
    "    results[ticker] = weights\n",
    "\n",
    "# Build DataFrame\n",
    "df_imf = pd.DataFrame(results, index=[f\"IMF{i+1}\" for i in range(NUM_IMFS)])\n",
    "\n",
    "if df_imf.empty:\n",
    "    logger.warning(\"âš ï¸ No IMF weights could be computed. Skipping plot.\")\n",
    "else:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bar_width = 0.12  # Smaller width since multiple stocks\n",
    "    x = np.arange(NUM_IMFS)\n",
    "\n",
    "    # Loop over tickers and plot bars\n",
    "    for i, ticker in enumerate(df_imf.columns):\n",
    "        plt.bar(\n",
    "            x + i * bar_width,\n",
    "            df_imf[ticker],\n",
    "            width=bar_width,\n",
    "            label=ticker,\n",
    "            alpha=0.85\n",
    "        )\n",
    "\n",
    "    # Axis labels and title\n",
    "    plt.xlabel(\"IMF Index\", fontsize=12)\n",
    "    plt.ylabel(\"Average Energy-Based Weight\", fontsize=12)\n",
    "    plt.title(\"Average Energy-Based IMF Weights (Across Fine-Tuned Models)\", fontsize=14)\n",
    "\n",
    "    # Center x-ticks\n",
    "    plt.xticks(x + (len(df_imf.columns) - 1) * bar_width / 2, df_imf.index)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show\n",
    "    plt.savefig(f\"{FIG_INT_DIR}/imf_energy_weights.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Console output of the DataFrame\n",
    "    print(df_imf.T.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0aae0-623f-428e-add0-4ace75b83fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
